# Local Deep Researcher Configuration
# Copy this file to .env and adjust the values as needed

# LLM Provider, either "ollama" or "lmstudio"
LLM_PROVIDER=ollama

# API key for Tavily
TAVILY_API_KEY=

# API key for Perplexity
PERPLEXITY_API_KEY=

# Local LLM model name (default if not specified: deepseek-r1:70b)
LOCAL_LLM=deepseek-r1:14b

# Ollama and LM Studio base URL 
OLLAMA_BASE_URL=http://localhost:11434/
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Search API: perplexity, tavily, duckduckgo, or searxng
SEARCH_API=duckduckgo

# Maximum number of web research loops
MAX_WEB_RESEARCH_LOOPS=5

# Whether to fetch full page content (for duckduckgo)
FETCH_FULL_PAGE=True

# Local RAG Configuration
USE_LOCAL_RAG=True
# Path to vector store directory (will find all vol_* folders automatically)
# You can also specify multiple base paths (comma-separated)
VECTOR_STORE_PATHS=/Users/ruiding/mac_python_folder/langchain_agent/vector_store
LOCAL_RESULTS_COUNT
EMBEDDING_MODEL=BAAI/bge-m3