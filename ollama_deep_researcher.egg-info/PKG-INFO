Metadata-Version: 2.4
Name: ollama-deep-researcher
Version: 0.0.1
Summary: Fully local web research and summarization assistant with Ollama and LangGraph.
Author: Lance Martin
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: langgraph>=0.2.55
Requires-Dist: langchain-community>=0.3.9
Requires-Dist: tavily-python>=0.5.0
Requires-Dist: langchain-ollama>=0.2.1
Requires-Dist: duckduckgo-search>=7.3.0
Requires-Dist: langchain-openai>=0.1.1
Requires-Dist: openai>=1.12.0
Requires-Dist: langchain_openai>=0.3.9
Requires-Dist: httpx>=0.28.1
Requires-Dist: markdownify>=0.11.0
Requires-Dist: python-dotenv==1.0.1
Requires-Dist: sentence-transformers==4.0.0
Requires-Dist: chromadb==0.6.3
Requires-Dist: peft==0.15.1
Provides-Extra: dev
Requires-Dist: mypy>=1.11.1; extra == "dev"
Requires-Dist: ruff>=0.6.1; extra == "dev"
Dynamic: license-file

# 🚀 Deploying the Local Deep Researcher Agent 🤖
(by Rui [Ray] Ding 🤓 2025-04-14; ruiding@uchicago.edu)
This guide outlines the steps to set up and run the local Deep Researcher (DR) agent developed during the 2025 Hackathon.

## Step 1: Setup Conda Environment via File 🐍📦

This project uses Conda to manage dependencies. The required environment, including specific package versions and necessary pip installations, is defined in the `DR_environment_Ray_0414.yml` file.

**1. Create the Environment:**

Ensure you have Conda installed. Then, from the root directory of this repository (where the `.yml` file is located), run:

```bash
conda env create -f DR_environment_Ray_0414.yml
```

This command will create a new conda environment named `hackathon-DR-agent` with Python 3.11 and all the necessary dependencies listed below.

**Contents of `DR_environment_Ray_0414.yml`:**

```yaml
name: hackathon-DR-agent
channels:
  - pytorch-nightly
  - defaults
dependencies:
  - python=3.11
  # Conda packages
  - pytorch
  - torchvision
  - torchaudio
  - selenium
  # Pip packages
  - pip
  - pip:
    - webdriver-manager
    - torch_geometric
    - scikit-learn
    - matplotlib
    # If your project needs to be installed in editable mode, uncomment the line below
    # (usually needed if the langgraph setup requires local package discovery)
    # - -e .
```

**2. Activate the Environment:**

Once the creation is complete, activate the environment:

```bash
conda activate hackathon-DR-agent
```

*(You'll need to keep this environment activated for the subsequent steps).*

## Step 2: Install and Setup Ollama 🧠

Install Ollama for running local Large Language Models (LLMs).

-   Ollama offers easy installation, especially on macOS and Linux.
-   Visit the official download page: [https://ollama.com/download/mac](https://ollama.com/download/mac)

After installing Ollama, pull the required LLMs:

```bash
# Recommended model
ollama pull deepseek-r1:14b

# Alternative for high-end machines (Mac Pro or better)
# ollama pull qwq
```

Test your local LLM setup by interacting with it directly through the Ollama interface.
e.g. in your terminal,
```bash
ollama run deepseek-r1:14b

```

## Step 3: Install and Configure `uvx` for LangGraph 🔗

Install `uvx` (a fast Python package installer/resolver):

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Use `uvx` to set up the LangGraph development environment.
**IMPORTANT:** This command uses `uvx` and actually it is creating its own env that is seperated from later `conda` or `pip` installs.

```bash
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev
```

You should see output similar to this, indicating the LangGraph server is running:
**SAMPLE OUTPUT ON MAC:**
(base) ruiding@ruidings-MacBook-Pro local-deep-researcher-ML-coder-v1 % curl -LsSf https://astral.sh/uv/install.sh | sh

downloading uv 0.6.14 aarch64-apple-darwin
no checksums to verify
installing to /Users/ruiding/.local/bin
  uv
  uvx
everything's installed!
(base) ruiding@ruidings-MacBook-Pro local-deep-researcher-ML-coder-v1 % uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev

      Built ollama-deep-researcher @ file:///Users/ruiding/mac_python_folder/langchain_agent/local-deep-researcher-ML-coder-v1
Installed 162 packages in 566ms
INFO:langgraph_api.cli:

        Welcome to


```
╦  ┌─┐┌┐┌┌─┐╔═╗┬─┐┌─┐┌─┐┬ ┬
║  ├─┤││││ ┬║ ╦├┬┘├─┤├─┘├─┤
╩═╝┴ ┴┘└┘└─┘╚═╝┴└─┴ ┴┴  ┴ ┴

- 🚀 API: http://127.0.0.1:2024
- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- 📚 API Docs: http://127.0.0.1:2024/docs

This in-memory server is designed for development and testing.
For production use, please use LangGraph Cloud.
```

**Note:** If the Studio UI link automatically opens in Safari, please copy and paste it into Chrome or Firefox for the best experience: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`

## Step 4: Setup Local RAG Vector Database 💾

**Critical Step:** This agent utilizes a local vector database for Retrieval-Augmented Generation (RAG). Due to its large size (~8GB), it cannot be included directly in the GitHub repository.

**1. Download the Database:**

*   Download the compressed vector database file (`vol_1.zip`) from the following link:
    *   **Google Drive Link:** [https://drive.google.com/file/d/1UYuS8zGzajhx30bQ1NlVJmWN28ow0Nkv/view?usp=sharing] *(Please contact me if you have trouble downloading it)*

**2. Prepare the Directory:**

*   Choose a location on your local machine to store the vector database.
*   **Example:** Let's assume you choose `/path/to/your/vector_store/`. Create this directory if it doesn't exist.

**3. Unzip the Database:**

*   Move the downloaded `vol_1.zip` file into the directory you chose (e.g., `/path/to/your/vector_store/`).
*   Navigate to that directory in your terminal and unzip the file:
    ```bash
    cd /path/to/your/vector_store/
    unzip vol_1.zip
    ```
*   (Optional but recommended) You can remove the zip file after successful extraction:
    ```bash
    rm vol_1.zip
    ```
    After unzipping, you should have the actual database files/folders directly within `/path/to/your/vector_store/`.

**4. Configure the Path:**

*   The agent needs to know where to find this database. You must update the path in **two** configuration files:
    *   `.env`: Find the `VECTOR_STORE_PATH` variable and set it to the **absolute path** of your vector store directory (e.g., `VECTOR_STORE_PATH=/path/to/your/vector_store/`).
    *   `configuration.py`: Find the `vector_store_paths` variable (likely a list) and update the path(s) accordingly (e.g., `vector_store_paths = ["/path/to/your/vector_store/"]`).
*   **Important:** Ensure the paths are correct and point to the directory containing the unzipped database files.

## Step 5: Configuring and Using the DR Agent via LangGraph Studio ⚙️

After starting the LangGraph server in Step 3, you can interact with the Deep Researcher agent through the LangGraph Studio UI (usually `http://127.0.0.1:2024` or the `smith.langchain.com` link).

**1. Select the Assistant/Graph:**

*   Once the UI loads, locate the **gear icon ⚙️** in the bottom left corner.
*   Click the gear icon to open the settings/configuration panel.
*   Select the specific graph (assistant) you want to use. The available graphs defined in this repository are:
    *   `Default_deep_researcher` (located in `ollama_deep_researcher` package)
    *   `ML_Code` (located in `ml_code_improver` package)

**2. Adjust Recursion Limit (Important!):**

*   Within the same settings panel for the selected graph, find the **"Recursion limit"** setting.
*   **What it is:** This controls the maximum number of steps or internal calls the agent can make during a single run, preventing potential infinite loops.
*   **Recommendation:** The default value might be low (e.g., 25), which is suitable for quick tests but often insufficient for deep research tasks. **It is highly recommended to increase this value.** A value of **300** was used during development, but adjust as needed based on your task complexity and available resources.
*   **How to Change:** Modify the number in the input field and ensure the change is saved/applied (the UI might do this automatically).
*   **Before submit deep research job, check how much limitation recurrence you give!**

## Step 6: Setup `auto_continue.py` for Long Runs ⏳

For long-running research tasks that might exceed the 1-hour limit in the LangGraph Studio UI, use the provided `auto_continue.py` script. This script monitors the UI in a separate browser window and automatically clicks the "Continue" button.

*(Dependencies for this script were installed via the `DR_environment_Ray_0414.yml` file in Step 1).*

**Running the auto-monitoring script:**

Navigate to the root directory of this repository and run (ensure your `hackathon-DR-agent` environment is active):

```bash
python auto_continue.py
```

This will open a new Chrome WebDriver window. You can interact with the LangGraph Studio UI in this window, and the script will handle clicking "Continue" automatically.

## Step 7: (optional) Local running GNN Examples ▶️

Now you can run the example projects. Ensure your `hackathon-DR-agent` conda environment is activated.

*(Dependencies for these examples were installed via the `DR_environment_Ray_0414.yml` file in Step 1).*

### Example 1: Cora Dataset Node Classification 📊

This example demonstrates Graph Neural Network (GNN) node classification on the Cora dataset.

**Running the example:**

Navigate to the `toy_cora_sample` directory (assuming it's in the root) and run the script:

```bash
# cd toy_cora_sample # Adjust if needed
python gnn_node_classification_cora.py
```

Expected output snippet:

```
Using MPS device
Starting training...
Epoch: 010, Loss: 0.8935, Train Acc: 0.9714, Val Acc: 0.7520, Test Acc: 0.7670
Epoch: 020, Loss: 0.2648, Train Acc: 0.9929, Val Acc: 0.7700, Test Acc: 0.7950
...
Epoch: 100, Loss: 0.0408, Train Acc: 1.0000, Val Acc: 0.7720, Test Acc: 0.7870
Training finished.
Final Train Accuracy: 1.0000
Final Validation Accuracy: 0.7720
Final Test Accuracy: 0.7870
```

### Example 2: Hackathon Chemistry Case 🧪

This example uses a GNN for a material characterization task from the hackathon.

**Prerequisites:**

-   I have already cloned the hackathon repository within this project structure. The code expects the path `ai-sci-hackathon-2025/material_characterize_project`.

**Running the example:**

1.  Navigate to the project directory:
    ```bash
    cd ai-sci-hackathon-2025/material_characterize_project
    ```
2.  Run the predictor script:
    ```bash
    python gnn_predictor.py
    ```

Expected output snippet:

```
Loading data...
/opt/anaconda3/envs/hackathon-DR-agent/lib/python3.11/site-packages/networkx/readwrite/json_graph/node_link.py:287: FutureWarning:
The default value will be changed to `edges="edges" in NetworkX 3.6.
...
Loaded 861 graphs.
Building vocabularies...
Found 53 unique atom types.
...
Creating fine-grained dataset...
Fine-grained training set size: 4220
Fine-grained validation set size: 974

Model Architecture:
GCNFineGrained(
...
)

Starting training...
Epoch: 001, Train Loss: 0.607522, Val Loss: 0.469003
  -> New best validation loss. Saved model checkpoint.
Epoch: 002, Train Loss: 0.450255, Val Loss: 0.413363
  -> New best validation loss. Saved model checkpoint.
...
Epoch: 019, Train Loss: 0.034244, Val Loss: 0.208273
  -> New best validation loss. Saved model checkpoint.
Epoch: 020, Train Loss: 0.037851, Val Loss: 0.216837
....
```

This indicates the model training has started.

---

**⚠️ Important Note on Execution Environment:**

While these examples can be run locally as shown, for more extensive or resource-intensive tasks (especially Step 7), consider using a dedicated high-performance computing environment (like RCC cluster, unluckily the DR agent could not be run on RCC because they do not support job nodes to connect internet) with properly configured Conda/Python environments for better performance and stability. This README provides a quick showcase of the setup and execution process.
