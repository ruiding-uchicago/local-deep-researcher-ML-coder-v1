# Local Deep Researcher Configuration

# LLM Provider, either "ollama" or "lmstudio"
LLM_PROVIDER=ollama

# API key for Tavily
TAVILY_API_KEY=

# API key for Perplexity
PERPLEXITY_API_KEY=

# Local LLM model name
LOCAL_LLM=deepseek-r1:14b

# Ollama and LM Studio base URL 
OLLAMA_BASE_URL=http://localhost:11434/
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Search API: perplexity, tavily, duckduckgo, or searxng
SEARCH_API=duckduckgo
LANGSMITH_API_KEY=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
# Maximum number of web research loops
#MAX_WEB_RESEARCH_LOOPS=3

# Whether to fetch full page content (for duckduckgo)
FETCH_FULL_PAGE=True

# Local RAG Configuration
USE_LOCAL_RAG=True
# Path to vector store directory (will find all vol_* folders)
VECTOR_STORE_PATHS=/Users/ruiding/mac_python_folder/langchain_agent/vector_store_cs/
LOCAL_RESULTS_COUNT=5
EMBEDDING_MODEL=BAAI/bge-m3

# Langgraph job configuration
BG_JOB_ISOLATED_LOOPS=true